{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Models and Axioms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability is used to draw conclusions (and hence make decisions) in an *uncertain* situation.\n",
    "\n",
    "Intuitively, we define probability of some uncertain event happening based on our past experience with similar events. For example, we know that tossing a coin, in general, returns head or tails half the time, so we say the probability is 50%. Thus we derive the probability based on the *frequency of occurence*\n",
    "\n",
    "Sometimes, we may have *subjective beliefs* about how probable something is - an example in Bertsekas is about a scholar who believes that Iliad and Odyssey were composed by the same person. \n",
    "\n",
    "A **probability model** is composed of :\n",
    "\n",
    "A **Sample Space** ($\\Omega$) composed of **outcomes**. The outcomes are *Mutually Exclusive, Collectively Exhaustive*.\n",
    "\n",
    "A **probability law** that assigns to any set A of possible outcomes (an **event** ), a non-negative number P(A) (the **probability** of A) that encodes our knowledge or belief about the likelihood of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The probability law must obey the **axioms of probability**\n",
    "\n",
    "* (Non-negativity) $P(A) >= 0$\n",
    "* (Additivity) If A and B are disjoint events (no shared outcomes), then $P(A \\cup B) = P(A) + P(B)$. Note that this can be easily extended to any finite number of disjoint sets.\n",
    "* (Normalization) $P(\\Omega) = 1$\n",
    "\n",
    "One way to think of the probability law is as a unit of mass spread over the sample space. P(A) is the total mass assigned to the elements of A. \n",
    "\n",
    "While the above sufficies for finite sample spaces, for sample spaces which are infinite in size, we need to extend the additivity axiom :\n",
    "* (Countable or $\\sigma$-Additivity) If $A_i$ are a sequence of disjoint events, then $P(\\bigcup A_i) = \\sum_i P(A_i)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Models\n",
    "\n",
    "**Discrete Probability Law**\n",
    "If a sample space consists of a finite number of outcomes, then every event can be considered as a union of disjoint sets, each containing one outcome. Thus,by the addivity axiom,\n",
    "\n",
    "$$P({s_1,s_2,...,s_n}) = P(s_1) + P(s_2) + ... + P(s_n)$$\n",
    "\n",
    "Here, $P(s_i)$ is a short form for $P({s_i})$\n",
    "\n",
    "**Discrete Uniform Probability Law**\n",
    "\n",
    "In the above instance, if we consider the case where all outcomes are equally likely, then for any event A,\n",
    "\n",
    "$P(A) = \\frac{{\\displaystyle|A|}} {{\\displaystyle|\\Omega|}} = \\frac{\\text{Number of Elements in A}}{\\text{Number of Elements in Sample Space}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Continuous Models\n",
    "\n",
    "In continuous sample spaces, or in discrete infinite spaces, the discrete probability law does not hold.\n",
    "\n",
    "Consider an example of choosing an integer at random? What is the probability it is even? What is the probability it is a particular number, say, 24? This is the same as the area of a point is zero, but we can talk of the non-zero area of triangle composed of the same points.\n",
    "\n",
    "Area, length and other concepts which involve integration are best explained by measure theory. The key point is that every subset of sample points is not necessarily a valid event. In practice, these examples are pathological, and we can proceed with the axioms as given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consequences of the Probability Axioms\n",
    "\n",
    "* (Empty Set) $P(\\emptyset) = 0$\n",
    "\n",
    "* (Complement) $P(A^c) = 1 - P(A)$\n",
    "\n",
    "* (Subset) $P(A) <= P(B) <= 1$ for $A \\subset B$\n",
    "\n",
    "* (Union-bound) $P(\\bigcup A_i) <= \\sum P(A_i)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "from __future__ import division\n",
    "\n",
    "def P(event, space): \n",
    "    \"The probability of an event, given a sample space of equiprobable outcomes.\"\n",
    "    return Fraction(len(event & space), len(space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = {1, 2, 3, 4, 5, 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraction(1, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even = {2,4,6}\n",
    "P(even,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mary and Tom park their cars in an empty parking lot with nâ‰¥2 consecutive parking spaces \n",
    "(i.e, n spaces in a row, where only one car fits in each space). \n",
    "Mary and Tom pick parking spaces at random. (All pairs of parking spaces are equally likely.) \n",
    "What is the probability that there is at most one empty parking space between them? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning and Bayes' Rule\n",
    "\n",
    "**Conditional Probability**\n",
    "\n",
    "Probability an event A has occured, given that some other event B has occured.\n",
    "\n",
    "$P(A | B) = \\frac{\\displaystyle{P(A \\cap B)}}{\\displaystyle{P(B)}}$\n",
    "\n",
    "Conditional probability for a fixed event B follow the probability axioms, and hence can be considered a new probability law on the same sample space, or as a probability law on a new sample space B.\n",
    "\n",
    "$P(\\Omega|B) = \\frac{P(\\Omega \\cap B)}{P(B)} = \\frac{P(B)}{P(B)} = 1$\n",
    "\n",
    "$P(A_1 \\cup A_2 | B) = \\frac{P(\\Omega \\cap B)}{P(B)} = \\frac{P(B)}{P(B)} = 1$\n",
    "\n",
    "** Multiplication Rule**\n",
    "As can be derived from the conditional probability definition (assuming P(B) > 0) :\n",
    "\n",
    "$P(A \\cap B) = P(B)P(A|B)$\n",
    "\n",
    "More generally, assuming all conditioning events have positive probability :\n",
    "\n",
    "$P(\\bigcap_i A_i) = P(A_1) \\prod_i P(A_i)*P(A_i| \\cap_{j=1}^{i-1} A_j)$\n",
    "\n",
    "** Total Probability Theorem **\n",
    "\n",
    "Let $A_1, A_2, ..., A_n$ be disjoint events that form a partition of the sample space, and assume that $P(A_i) > 0$, for all i. Then, for any event B such that P(B) > 0, we have \n",
    "\n",
    "$P(B) = \\sum_i P(A_i)P(B|A_i)$\n",
    "\n",
    "** Inference and Bayes' Rule**\n",
    "\n",
    "$P(A_i | B) = \\frac{\\displaystyle{P(A_i)P(B | A_i)}}{\\displaystyle{P(B)}} = \\frac{\\displaystyle{P(A_i)P(B | A_i)}}{\\displaystyle{\\sum_i P(A_i)P(B|A_i)}}$\n",
    "\n",
    "One way to think of this is as follows : Think of the A's as the *causes* and B as the *effect*. We know the conditional probability of B occuring, given the A's have occured, and the probability of the A's themselves.\n",
    "\n",
    "We now want to know that, *given B has occured*, what can we *infer* about the probability that a particular A has occured - i.e. that a particular A is the cause of the effect. Remember that the A's are disjoint, so only one cause may really have occured.\n",
    "\n",
    "$P(A_i|B)$ is called the **posterior probability** of event $A_i$, as opposed to $P(A_i)$, which is called the **prior probability**.\n",
    "\n",
    "### Example of Medical Tests\n",
    "\n",
    "A classic case of the use of Bayes theorem are medical test results.\n",
    "\n",
    "A disease usually occurs in a small fraction of the population. Say 5% of people doing routine cancer tests have cancer. 99% of people who have cancer and are tested have positive tests, 1% are \"missed\" (a false negative). On the other hand, 10% of people who do not have cancer, are tested positive (a false positive). \n",
    "\n",
    "Question: If a person gets a positive test, what are the chances the person has cancer?\n",
    "\n",
    "Here B = getting a positive test. A1 = person has cancer, A2 = person does not have cancer.\n",
    "\n",
    "The prior probabilities are (disjoint/exhaustive)\n",
    "P(A1) = 0.05\n",
    "P(A2) = 0.95\n",
    "\n",
    "Also, we know P(B|A1) = 0.99, P(B|A2) = 0.10\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(A1|B) & = \\frac{P(A1)P(B|A1)}{P(A1)P(B|A1) + P(A2)P(B|A2)} \\\\[2ex]\n",
    "& = 0.05*0.99/(0.05*0.99 + 0.95*0.10) \\\\[2ex] \n",
    "& = 34.2\\% \n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "The surprising result comes from the fact that a large percentage of people don't have cancer, so even a test as sensitive as this gives a large absolute number of false positives.\n",
    "\n",
    "For more information see [Wikipedia - Sensitivity and Specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence\n",
    "\n",
    "Two events are said to be independent if the knowledge that one has occured does not give any information about the occurence of the other event i.e. $P(A|B) = P(A)$\n",
    "\n",
    "From this, we can see that if A and B are independent :\n",
    "\n",
    "$P(A \\bigcap B) = P(A)P(B)$\n",
    "\n",
    "This definition works correctly even when P(B) is zero i.e. P(A|B) is undefined. \n",
    "\n",
    "Note that disjoint events are not independent - in fact, they are the opposite. If one occurs, we know the other has not occured. If two events are independent, and have non-zero probability, then they must be intersecting i.e. have some sample points in common. \n",
    "\n",
    "We can extend this definition to **conditional independence**.\n",
    "\n",
    "$P(A \\bigcap B | C) = P(A|C)P(B|C)$\n",
    "\n",
    "However, it is important to note that two events may be conditional independent, but not unconditionally so, and vice versa.\n",
    "\n",
    "**independence of several events** are independent if all possible combinations of the events satisfy the definition :\n",
    "\n",
    "Let A = {A_1, A_2, ..., A_n} be a set of events. These events are said to be independent if :\n",
    "\n",
    "$P(\\bigcap_{i \\in S}) = \\prod_{i \\in S} P(A_i)$ where S is any subset of {1,2,..n} with 2 or more numbers i.e. $2^n - n - 1$}$ equations are needed for n events.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting\n",
    "\n",
    "## Basic Counting Principle\n",
    "\n",
    "* r stages\n",
    "* $n_i$ choices at stage 1\n",
    "No of combinations = $n_1*n_2*..n_r$\n",
    "\n",
    "**Examples**\n",
    "\n",
    "* License Plates : 2 letters, followed by 3 digits\n",
    "\n",
    "26*26*10*10*10\n",
    "\n",
    "* License Plates : 2 letters, followed by 3 digits - no repetition\n",
    "\n",
    "26*25*10*9*8\n",
    "\n",
    "## Permutations\n",
    "\n",
    "* (Permutation) Number of ways of ordering n elements, by counting principle\n",
    "\n",
    "n*(n-1)*(n-2)...1 = n!\n",
    "\n",
    "* No of subsets of (1..n):\n",
    "  2.2.2..2 = $2^n$\n",
    "\n",
    "## Combinations\n",
    "\n",
    "number of k-element subsets of a given n-element set\n",
    "\n",
    "$C(n,k) = \\frac{\\displaystyle{n!}}{\\displaystyle{n-k!k!}}$\n",
    "\n",
    "$\\sum_{k=1}^{n} C(n,k) = 2^n$ - This makes sense since this is the total number of subsets of a set.\n",
    "\n",
    "## Binomial Probabilities\n",
    "\n",
    "n >= 1 independent coin tosses.\n",
    "P(H) = p\n",
    "\n",
    "P(particular k-head sequence) = $p^k(1-p)^{n-k}$ \n",
    "\n",
    "P(any k head-sequence) = P(particular k-head sequence) * #k-head sequences \n",
    "\n",
    "#k-head sequence = no of ways to have k-sized subsets of a set of size n = C(n,k)\n",
    "\n",
    "P(any k head-sequence) = $p^k*(1-p)^{n-k}*C(n,k)$\n",
    "\n",
    "## Partitions\n",
    "\n",
    "* n >= 1 distinct items\n",
    "* r >= 1 persons each getting $n_i$ items to exhaust all items.\n",
    "\n",
    "\n",
    "How many ways can this be done? Let this be equal to c.\n",
    "\n",
    "Let us assume we create an ordered list of the n original items by asking each person i to order their $n_i$ items in all possible ways, one after another. Then we get the equality :\n",
    "\n",
    "$c.n_1!.n_2!..n_r! = n!$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$c = n!/n_1!.n_2!..n_r!$\n",
    "\n",
    "This is called the multinomial coefficient.\n",
    "\n",
    "\n",
    "r = 2 : Binomial Coefficient, which we discussed earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Random Variables\n",
    "\n",
    "A **random variable** (r.v.) is a numerical quantity whose value is determined by a probabilistic experiment. E.g. the height of a randomly selected student. Essentially it is a function which takes an outcome as an input, and provides some value (a real number) as an output.\n",
    "\n",
    "Examples :\n",
    "\n",
    "* Number of heads in 10 consecutive coin tosses (discrete r.v.)\n",
    "\n",
    "* Measurement of the time at which something happened (continuous r.v.)\n",
    "\n",
    "First focus is on Discrete Random Variables only i.e. which are based on a finite or countably infinite sample spaces.\n",
    "\n",
    "A function of one or more random variables is also a random variable e.g.\n",
    "\n",
    "BMI = Height / Weight^2 is a random variable that is a function of 2 other r.v.'s - height and weight.\n",
    "\n",
    "### Probability Mass Function\n",
    "\n",
    "PMF of a discrete r.v. X\n",
    "\n",
    "Probability distribution of X - a probability law, or a probability measure that determines the distribution of the values of X.\n",
    "\n",
    "$p_X(x) = P(X = x) = P({w \\in \\Omega.:X(w) = x})$\n",
    "\n",
    "This should follow all the probability axioms.\n",
    "\n",
    "**Classic Example - Dice**\n",
    "\n",
    "2 die. X = value of die 1, Y = value of die 1\n",
    "\n",
    "Let Z = X + Y be a random variable.\n",
    "\n",
    "**Bernoulli R.V.**\n",
    "\n",
    "X = 1 with probability p, or 0 with probability 1-p.\n",
    "\n",
    "Models a trial that results in success or failure, heads or tails etc. It acts like an indicator r.v for an event A.\n",
    "\n",
    "$p_{I_A}(1) = P(I_A = 1) = P({w \\in \\Omega: w \\in A})$\n",
    "\n",
    "**Discrete Uniform Random Variable**\n",
    "\n",
    "One which takes on a set of integer values in a range, with equal probability.\n",
    "\n",
    "Models complete ignorance. E.g. the value of seconds when we look at a clock.\n",
    "\n",
    "Parameters : a, b - which are the range of possible values. \n",
    "\n",
    "Number of values = b - a + 1.\n",
    "\n",
    "Probability of each value = 1/b-a+1.\n",
    "\n",
    "**Binomial Random Variable**\n",
    "\n",
    "Experiment : n independent coin tosses of a coin with P(Heads) = p\n",
    "\n",
    "Sample space : Sequences of H and T, of length n.\n",
    "\n",
    "Random variable X = no of Heads observed.\n",
    "\n",
    "$p_X(k) = C(n,k).p^k(1-p)^{n-k}$\n",
    "\n",
    "Can model any experiment where number of successes in a given number of independent trials.\n",
    "\n",
    "**Geometric Random Variable**\n",
    "\n",
    "Experiment : infinitely many independent tosses of a coin P(Heads) = p\n",
    "\n",
    "Sample Space : Set of infinite sequences of H and T.\n",
    "\n",
    "Random Variable X : number of tosses until the first heads.\n",
    "\n",
    "$p_X(k) = P(X = k) = (1-p)^{k-1}p$\n",
    "\n",
    "P(no heads ever) = 0, in the limit.\n",
    "\n",
    "Models a \"waiting for success\" event where we repeat an experiment till we succeed.\n",
    "\n",
    "## Expectation of a Random Variable\n",
    "\n",
    "$E[X] = \\sum_x x*p_X(x)$\n",
    "\n",
    "Assumes for infinite case the expectation is finite i.e series converges.\n",
    "\n",
    "Expected value of a Bernoulli r.v (not infinite, so quite easy):\n",
    "\n",
    "E[X] = 1.p + 0.(1-p) = p\n",
    "\n",
    "Expected value of an indicator R.V. for an event A, X = $I_A$\n",
    "E[$I_A] = P(A)\n",
    "\n",
    "e.v of Uniform Random Variable uniform on 0-n.\n",
    "\n",
    "p(x) = 1/n+1\n",
    "\n",
    "E[X] = 0.(1/n+1) + 1.(1/n+1) ... n/n+1)\n",
    "     = 1/n+1(0 + 1 + ..n) = (1/n+1)(n.(n+1)/2)\n",
    "     = n/2\n",
    "     \n",
    "Center or midpoint. This is normal for any symmetric distribution. More generally it is like the center of gravity.\n",
    "\n",
    "Expectation as the average of a population :\n",
    "\n",
    "n students, weight $x_i$ for student i. Experiment : pick a student, all equally likely.\n",
    "\n",
    "R.v : X : Weight of selected student (assume x_i are distinct).\n",
    "\n",
    "$E[X] = \\frac{\\sum_i x_i}{n}$\n",
    "\n",
    "**Some Properties**\n",
    "\n",
    "* If X >= 0, E[X] >= 0\n",
    "\n",
    "* More generally, if a <= X <= b, a <= E[X] <= b\n",
    "\n",
    "* Specifically, if a = b, then :\n",
    "E[c] = c\n",
    "\n",
    "**Expected Value Rule**\n",
    "\n",
    "Let X = r.v.\n",
    "\n",
    "Y = g(X)\n",
    "\n",
    "E[Y] = Sum y.pY(y)\n",
    "\n",
    "But another way to think :\n",
    "\n",
    "E[Y] = E[g(X)] = Sum g(x) pX(x)\n",
    "\n",
    "**Linearity of Expectations**\n",
    "\n",
    "E[aX + b] = Sum g(x) pX(x) = Sum (ax + b).pX(x)\n",
    "          = a Sum x.pX(x) + b Sum pX(x)\n",
    "          = a E[X] + b\n",
    "          \n",
    "E[g(X)] = g(E[X]) for linear functions.\n",
    "\n",
    "What is g a function of?\n",
    "\n",
    "What if g(X) = c?\n",
    "\n",
    "E[c] = c.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance\n",
    "\n",
    "Variance - a measure of the spread of a PMF\n",
    "\n",
    "* Random variable with mean $\\mu = E[X]$\n",
    "* Distance from mean = X - $\\mu$\n",
    "* $E[X - \\mu] = E[X] - \\mu = \\mu = \\mu = 0$ => that average of distance from mean is always 0.\n",
    "\n",
    "\n",
    "$var(X) = E[(X - \\mu)^2] = E[X^2 - 2X\\mu + \\mu^2] = E(X^2) - (E(X))^2$ knowing that $\\mu$ = E[X]\n",
    "\n",
    "$var(aX + b) = a^{2}var(X)$\n",
    "\n",
    "**Bernoulli r.v.**\n",
    "\n",
    "var(X) = E[X^2] - E(X)^2 = E[X] - E(X)^2 = p(1-p)\n",
    "\n",
    "highest value when p = 1-p ie. p = 0.5. This can be considered a case of max randomness - when a coin is fair.\n",
    "\n",
    "**Uniform r.v.**\n",
    "\n",
    "* X is a r.v. with values 0..n\n",
    "\n",
    "var(X) = E[X^2] - E(X)^2\n",
    "\n",
    "E[X^2] = 1/n+1[0^2 + 1^2 + ... + n^2]\n",
    "\n",
    "var(X) = (1/12).n.(n+2)\n",
    "\n",
    "* X is a r.v. with values a..b, b >= a\n",
    "\n",
    "var(X) = var(X-a) = (1/12)(b-a).(b-a+2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional PMFs and Expectations\n",
    "\n",
    "Like ordinary PMFs, but all probabilities are conditioned.\n",
    "\n",
    "Total Expectation Theorem (as a parallel to the Total Probability Theorem)\n",
    "\n",
    "$E[X] = P(A_1)E[X|A_1] + ... + P(A_n)E[X|A_n]$\n",
    "\n",
    "**Geometric PMF, Memorylessness, Expectation**\n",
    "\n",
    "Memorylessness - past does not impact future coin tosses.\n",
    "\n",
    "If you ignore the first coin toss, the remaining distribution is still geometric with parameter p.\n",
    "\n",
    "Conditioned on X > 1, X-n is geometric with parameter p.\n",
    "\n",
    "$P_{X-n|X>n}(k) = p_X(k)$\n",
    "\n",
    "E[X] = 1 + E[X-1] (since there has to be at least one toss).\n",
    "\n",
    "E[X-1] = p.E[X-1|X=1] + (1-p) E[X-1|X>1]\n",
    "..       = 0 + (1-p)E[X] (by memorylessness\n",
    "\n",
    "E[X] = 1 + (1-p) E[X]\n",
    "\n",
    "E[X] = 1/p\n",
    "\n",
    "**Var(X) for a Geometric Distribution**\n",
    "\n",
    "Given X > 1, X-1 has same geometric PMF. Thus condition PMF of X given X > 1, is same as unconditional PMF of X+1.\n",
    "\n",
    "E[X|X>1] = 1 + E[X], E[X^2 | X > 1] = E[(X+1)^2]\n",
    "\n",
    "Var(X) = E(X^2) - (E(X))^2\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(X^2) & = P(X=1)P(E(X^2|X=1) + P(X>1)E[X^2|X > 1] \\\\\n",
    "& = p + (1-p)(E(X^2) + 2E[X] + 1) \\\\\n",
    "& = p + (1-p)(E[X^2] + 2/p + 1) \\\\\n",
    "E[X^2] & = \\frac{2-p}{p^2} \\\\\n",
    "var(X) & = \\frac{1-p}{p^2}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple random variable and joint PMFs\n",
    "\n",
    "\n",
    "Joint PMF : $p_{X,Y}(x,y) = P(X=x \\land Y=y)$\n",
    "\n",
    "$\\sum_x\\sum_yp_{X,Y}(x,y) = 1$\n",
    "\n",
    "Can be extended to multiple r.v.s\n",
    "\n",
    "Functions of multiple random variables\n",
    "\n",
    "Z = g(X,Y)\n",
    "\n",
    "$p_Z(z) = P(Z=z) = P(g(X,Y) = z) = \\sum_{(x,y):g(x,y)=z}P_{X,Y}(x,y)$\n",
    "\n",
    "Expected Value Rule :\n",
    "\n",
    "$E[g(X,Y)] = \\sum_x \\sum_y g(x,y)p_{X,Y}(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional PMFs\n",
    "\n",
    "$p_{X|Y}(x|y)=\\frac{\\displaystyle{p_{X,Y}(x,y)}}{\\displaystyle{p_Y(y)}}$\n",
    "\n",
    "Version of Multiplication Rule:\n",
    "\n",
    "$ p_{X,Y}(x,y) = p_Y(y)p_{X|Y}(x|y)$\n",
    "\n",
    "$ p_{X,Y}(x,y) = p_X(x)p_{Y|X}(y|x)$\n",
    "\n",
    "\n",
    "### Total Probability and Expectation Theorems\n",
    "\n",
    "$p_X(x) = \\sum_y p_Y(y)p_{X|Y}(x|y)$\n",
    "\n",
    "$E[X] = \\sum_y p_Y(y)E[X|Y=y]$\n",
    "\n",
    "These equations are valid even when Y is discrete but infinite, but requires more proof, as long as E[X] is well defined i.e. converges.\n",
    "\n",
    "### Independence of r.v.s\n",
    "\n",
    "$p_X|A(x) = p_X(x)$ for all x, i.e. A does not change distribution of X.\n",
    "\n",
    "$p_X,Y,Z(x,y,z) = p_X(x)p_Y(y)p_Z(z)$ for all x,y,z\n",
    "\n",
    "In general,\n",
    "\n",
    "E[g(X,Y)] <> g(E[X], E[Y]), except for linearity, and sums.\n",
    "\n",
    "\n",
    "If X,Y are independent : E[XY] = E[X]E[Y]\n",
    "\n",
    "and g(X) and h(Y) are independent too.\n",
    "\n",
    "E[g(X)h(Y)] = E[g(X)].E[h(Y)]\n",
    "\n",
    "If X and Y are independent,\n",
    "\n",
    "E[X/Y] = E[X].E[1/Y], not E[X]/E[Y], which may be very different.\n",
    "\n",
    "### Independence, Variances and binomial variance\n",
    "\n",
    "$var(aX+b) = a^2var(X)$\n",
    "\n",
    "in general var(X+Y) <> var(X) + var(Y)\n",
    "\n",
    "If X,Y are indepenent : \n",
    "\n",
    "$var(X+Y) = var(X) + var(Y)$\n",
    "\n",
    "Variance of a binomial - no of successes in n indepdendent trials\n",
    "\n",
    "$X_i$ = 1 if ith trial is a success, else 0\n",
    "\n",
    "$X_i$s are independent.\n",
    "\n",
    "$X = \\sum_i X_i$ is a sum of independent variables.\n",
    "\n",
    "Hence, $var(X) = \\sum_i var(X_i) = \\sum_i p(1-p) = n.p(1-p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hat Problem\n",
    "\n",
    "n people, throw their hats in a box, and then pick one at random.\n",
    "\n",
    "All permutations equally likely\n",
    "Equivalent to picking one at a time\n",
    "\n",
    "X: number of people who get their own hat\n",
    "\n",
    "What is E[X]?\n",
    "\n",
    "By definition: $E[X] = \\sum_k k p_X(k)$. But what is $p_X(k)$?\n",
    "\n",
    "Instead we rephrase. Let $X_i$ be an indicator variable that is 1 when the ith person gets own hat back, else it is 0. Then $X = \\sum X_i$. \n",
    "\n",
    "what is $E[X_i]$?\n",
    "\n",
    "$E[X_i] = 1.p(pick own hat) + 0.p(not pick own hat) = 1/n$\n",
    "\n",
    "E[X] = 1/n.n = 1.\n",
    "\n",
    "var(X). Easy if X_i independent. But they are not (think of 2 people case).\n",
    "\n",
    "$var(X) = E[X^2] - (E[X])^2 = 2$ by using symmetry arguments.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "**4 Buses with students = {40,33,25,50}**\n",
    "\n",
    "148 students\n",
    "\n",
    "a.one student is selected at random. \n",
    "X = # of students in bus of the selected student\n",
    "\n",
    "$p(B_i) = \\sum s_i^2/148$\n",
    "E(X) = 39.28\n",
    "\n",
    "b. Select driver/bus at random.\n",
    "Y: ... driver.\n",
    "\n",
    "37\n",
    "\n",
    "**From tail probs to expectations**\n",
    "\n",
    "X : nonnegative integer values\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[X] & = \\sum_{k=1}^{\\infty}P(X \\geq k) \\\\\n",
    "& =  \\sum_{k=1}^{\\infty}\\sum_{j=k}^{\\infty}p_X(j) \\\\\n",
    "& = \\sum_{j=1}^{\\infty}\\sum_{k=1}^{i}p_X(j) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Coupon Collectors problem**\n",
    "\n",
    "How many coupons do you have to collect till you see all k coupons, assuming on each trial you get each coupon equally likely.\n",
    "\n",
    "Z = X + Y\n",
    "Geometric r.v.s with probability p.\n",
    "\n",
    "X=i : probability of heads in ith trial.\n",
    "X+Y=n : probability of heads in 2 different trials need a total of n  flips of coin - e.g. if n = 10, then X=1, Y=9 ... etc.\n",
    "\n",
    "\n",
    "P(X=i|X+Y=n) = P(X=i).P(Y=n-i)/P(X+Y=n)\n",
    "\n",
    "**Inclusion/Exclusion formula**\n",
    "\n",
    "P(Union of n events) = Sum(P(events) - Sum(P(2 events at a time) + Sum(P(3 events at a time) + (-1)^(n-1) Sum (P(all n events)\n",
    "\n",
    "Great solution using indicator variables.\n",
    "\n",
    "**Independence of random variables vs independence of events**\n",
    "n events are independent iff the n indicator random variables are indepdendent.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A : 6 heads in first 8 tosses\n",
    "B : 9th toss is head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "    - Conditional Expectation; Examples\n",
    "    - Multiple Discrete Random Variables\n",
    "    - Continuous Random Variables \n",
    "    - Continuous Random Variables and Derived Distributions\n",
    "    - Convolution\n",
    "    - Transforms\n",
    "    - Iterated Expectations\n",
    "    - Sum of a Random Number of Random Variables\n",
    "    - Prediction; Covariance and Correlation\n",
    "    - Weak Law of Large Numbers\n",
    "    - Bernoulli Process\n",
    "    - Poisson Process\n",
    "    - Poisson Process Examples\n",
    "    - Markov Chains\n",
    "    - Central Limit Theorem\n",
    "    - Strong Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "[Probability, Paradox and the Reasonable Person Principle](http://nbviewer.jupyter.org/url/norvig.com/ipython/Probability.ipynb)\n",
    "\n",
    "[MITx: 6.041x Introduction to Probability - The Science of Uncertainty](https://courses.edx.org/courses/course-v1:MITx+6.041x_3+2T2016/)\n",
    "\n",
    "[Mathjax Quick Reference](http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)\n",
    "\n",
    "[The origins and legacy of Kolmogorovâ€™s Grundbegriffe](http://www.probabilityandfinance.com/articles/04.pdf)\n",
    "\n",
    "[Discrete Schotastic Processes](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/video-lectures/lecture-1-introduction-and-probability-review/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
